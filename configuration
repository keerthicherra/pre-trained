from transformers import LlamaConfig
config = LlamaConfig()
print(config)

config.num_hidden_layers = 12      # reduced from 32 to 12
config.hidden_size = 1024          # reduced 1/4 from 4096 to 1024
config.intermediate_size = 4096    # reduced 1/3 from 11008 to 4096 (dimension of MLP representations)
config.num_key_value_heads = 8     # reduced 1/4 from 32 to 8 (defaults to num_attention_heads=32)
config.torch_dtype = "bfloat16"    # for half-precision training
config.use_cache = False           # `True` is incompatible w/ gradient checkpointing
print(config)

from transformers import LlamaForCausalLM
model = LlamaForCausalLM(config)
print(model)

def print_nparams(model):
    """Calculate the total number of model parameters"""
    nparams = sum(p.numel() for p in model.parameters())
    print(f"The total number of parameters is: {nparams}")

print_nparams(model)  # 248013824 => 248M

layer_name = "model.layers.0.self_attn.q_proj.weight"

for name, param in model.named_parameters():
    if name == layer_name:
        print(f"First 30 weights of layer '{layer_name}':")
        print(param.data.view(-1)[:30])
        break

!pip install huggingface_hub transformers

from huggingface_hub import notebook_login
from transformers import LlamaTokenizer, LlamaForCausalLM
import torch

# Step 1: Log into Hugging Face account (ensure you have a valid token for private models)
notebook_login()

# Alternatively, if notebook_login() doesn't work for you, use this:
# token = "your_huggingface_token"  # Replace with your actual Hugging Face token
# You can get the token from https://huggingface.co/settings/tokens

# Step 2: Verify that the model path is correct
model_dir = "upstage/solar-docvision-preview"  # Ensure this model exists and is accessible to you

# Step 3: Load the tokenizer and model
try:
    tokenizer = LlamaTokenizer.from_pretrained(model_dir)
    model = LlamaForCausalLM.from_pretrained(model_dir).to("cuda" if torch.cuda.is_available() else "cpu")
except OSError as e:
    print(f"Error loading model: {e}")
    # If you encounter an error, it may be due to a private or non-existent repository
    # If it's a private repo, ensure you are authenticated properly

# Step 4: Define a prompt for text generation
prompt = "I am an engineer. I love"

# Step 5: Tokenize the input prompt
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# Step 6: Generate the output from the model
try:
    outputs = model.generate(
        **inputs,
        max_new_tokens=128,
        do_sample=False,  # Use deterministic output
        use_cache=True,
    )

    # Decode the generated tokens back to text
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print("Generated Text:", generated_text)

except Exception as e:
    print(f"Error during text generation: {e}")

# NOTE: We're running large models in a limited environment. Run me if you encounter any memory issues.
import gc

# Check if 'model' exists before trying to delete it
if 'model' in globals():
    del model

# 'streamer' is not defined, so remove this line
# del streamer

# Check if 'outputs' exists before trying to delete it
if 'outputs' in globals():
    del outputs

gc.collect()

