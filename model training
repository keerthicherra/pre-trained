import warnings
warnings.filterwarnings('ignore')

## 1. Load the model to be trained
import torch
from transformers import AutoModelForCausalLM

pretrained_model = AutoModelForCausalLM.from_pretrained(
    "facebook/opt-125m", # Example model ID from Hugging Face Hub
    device_map="cpu",
    torch_dtype=torch.bfloat16,
    use_cache=False,
)

## 2. Load dataset
!pip install datasets
import datasets
from torch.utils.data import Dataset

class CustomDataset(Dataset):
    def __init__(self, args, split="train"):
        """Initializes the custom dataset object."""
        self.args = args
        self.dataset = datasets.load_dataset(
            "parquet",
            data_files=args.dataset_name,
            split=split
        )

    def __len__(self):
        """Returns the number of samples in the dataset."""
        return len(self.dataset)

    def __getitem__(self, idx):
        """
        Retrieves a single data sample from the dataset
        at the specified index
        """
        # Convert the lists to a LongTensor for PyTorch
        input_ids = torch.LongTensor(self.dataset[idx]["input_ids"])
        labels = torch.LongTensor(self.dataset[idx]["input_ids"])

        # Return the sample as a dictionary
        return {"input_ids": input_ids, "labels": labels}


## 3. Configure Training Argument
from dataclasses import dataclass, field
import transformers
from transformers import HfArgumentParser
import torch

@dataclass
class CustomArguments(transformers.TrainingArguments):
    dataset_name: str = field(
        default="./parquet/packaged_pretrain_dataset.parquet"
    )
    num_proc: int = field(default=1)
    max_seq_length: int = field(default=32)

    seed: int = field(default=0)
    optim: str = field(default="adamw_torch")
    max_steps: int = field(default=30)
    per_device_train_batch_size: int = field(default=2)

    learning_rate: float = field(default=5e-5)
    weight_decay: float = field(default=0)
    warmup_steps: int = field(default=10)
    lr_scheduler_type: str = field(default="linear")
    gradient_checkpointing: bool = field(default=True)
    dataloader_num_workers: int = field(default=2)
    bf16: bool = field(default=True)
    gradient_accumulation_steps: int = field(default=1)

    logging_steps: int = field(default=3)
    report_to: str = field(default="none")

    # Uncomment and modify these lines if needed for checkpoint saving
    # save_strategy: str = field(default="steps")
    # save_steps: int = field(default=3)
    # save_total_limit: int = field(default=2)

# Initialize argument parser and parse arguments
parser = HfArgumentParser(CustomArguments)
args, = parser.parse_args_into_dataclasses(
    args=["--output_dir", "output"]
)

# Define a simple CustomDataset class if you don't have one already
class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, args: CustomArguments):
        # Here you would load and preprocess your dataset according to `args`
        self.args = args
        self.data = self.load_data()

    def load_data(self):
        # Implement loading of dataset here
        # For example: return a list of dictionary items
        return [{"input_ids": torch.randint(0, 100, (self.args.max_seq_length,))} for _ in range(100)]

    def __getitem__(self, idx):
        return self.data[idx]

    def __len__(self):
        return len(self.data)

# Instantiate the dataset
train_dataset = CustomDataset(args=args)

# Print the shape of a sample input
print("Input shape:", train_dataset[0]['input_ids'].shape)


## 4. Run the trainer and monitor the loss
import os
from dataclasses import dataclass, field
import torch
from transformers import Trainer, TrainingArguments, TrainerCallback, AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
import transformers

# Custom Arguments
@dataclass
class CustomArguments(TrainingArguments):
    dataset_name: str = field(default="wikitext", metadata={"help": "Name of the dataset"})
    dataset_config: str = field(default="wikitext-2-raw-v1", metadata={"help": "Configuration of the dataset"})
    num_proc: int = field(default=1)
    max_seq_length: int = field(default=128)
    seed: int = field(default=0)
    optim: str = field(default="adamw_torch")
    max_steps: int = field(default=30)
    per_device_train_batch_size: int = field(default=2)
    learning_rate: float = field(default=5e-5)
    weight_decay: float = field(default=0.01)
    warmup_steps: int = field(default=10)
    lr_scheduler_type: str = field(default="linear")
    gradient_checkpointing: bool = field(default=True)
    dataloader_num_workers: int = field(default=2)
    bf16: bool = field(default=False)  # Use bf16 if your hardware supports it
    gradient_accumulation_steps: int = field(default=1)
    logging_steps: int = field(default=3)
    report_to: str = field(default="none")

# Custom Dataset
class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, args, tokenizer):
        self.args = args
        # Load dataset from Hugging Face
        dataset = load_dataset(args.dataset_name, args.dataset_config)
        self.data = dataset['train']
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        text = item['text']  # Access raw text from the dataset
        # Tokenize the text and truncate to max sequence length
        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.args.max_seq_length)
        input_ids = torch.tensor(encoding['input_ids'])
        labels = input_ids.clone()  # For language models, labels can be the same as input_ids
        return {'input_ids': input_ids, 'labels': labels}

# Custom Callback for logging loss
class LossLoggingCallback(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is not None:
            print("Loss:", logs.get("loss"))

# Initialize tokenizer
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# Set `pad_token` to `eos_token` (GPT-2 does not have a pad token by default)
tokenizer.pad_token = tokenizer.eos_token

# Set up arguments and dataset
parser = transformers.HfArgumentParser(CustomArguments)
args, = parser.parse_args_into_dataclasses(args=["--output_dir", "output", "--per_device_train_batch_size", "2", "--learning_rate", "5e-5", "--max_steps", "30"])

# Load the dataset with the tokenizer
train_dataset = CustomDataset(args=args, tokenizer=tokenizer)

# Load pre-trained model (GPT-2 in this case)
pretrained_model = AutoModelForCausalLM.from_pretrained("gpt2")

# Resize the token embeddings to account for the new pad token if necessary
pretrained_model.resize_token_embeddings(len(tokenizer))

# Initialize the callback for logging loss
loss_logging_callback = LossLoggingCallback()

# Set up the Trainer
trainer = Trainer(
    model=pretrained_model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=None,  # Optional: Add evaluation dataset if needed
    callbacks=[loss_logging_callback]
)

# Start training
trainer.train()

# Text Generation with GPT-2
model_name_or_path = "gpt2"  # You can change this to another model if needed
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
model2 = AutoModelForCausalLM.from_pretrained(model_name_or_path)

# Generation prompt
prompt = "I am an engineer. I love"
inputs = tokenizer(prompt, return_tensors="pt").to(model2.device)

# Generate text
outputs = model2.generate(
    inputs['input_ids'],
    max_new_tokens=50,
    do_sample=False,  # Disable sampling to get deterministic results
    temperature=1.0
)

# Decode and print generated text
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("Generated text:", generated_text)

  from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer

# Initialize tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2").to("cuda" if torch.cuda.is_available() else "cpu")

# Prompt for text elaboration
prompt = "The future of artificial intelligence lies in"

# Tokenize the input prompt
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# Initialize TextStreamer for streaming output (optional for real-time viewing)
streamer = TextStreamer(
    tokenizer,
    skip_prompt=True,
    skip_special_tokens=True
)

# Generate elaborated text using the model
outputs = model.generate(
    **inputs,
    streamer=streamer,  # Stream the generated output
    use_cache=True,
    max_new_tokens=300,  # Allow for up to 100 new tokens for elaboration
    do_sample=True,  # Enable sampling for more diverse output
    temperature=1.2,  # Increase temperature to make output more creative
    top_p=0.9,  # Use nucleus sampling to focus on the most probable words
    repetition_penalty=1.2,  # Add a penalty for repeated phrases
)

# The streamer will print the generated text in real-time, so no need to decode manually.

# Lesson 6. Model evaluation
!pip install -U git+https://github.com/EleutherAI/lm-evaluation-harness    
!lm_eval --model hf \
    --model_args pretrained=./models/TinySolar-248m-4k \
    --tasks truthfulqa_mc2 \
    --device cpu \
    --limit 5

    import os

def h6_open_llm_leaderboard(model_name):
  task_and_shot = [
      ('arc_challenge', 25),
      ('hellaswag', 10),
      ('mmlu', 5),
      ('truthfulqa_mc2', 0),
      ('winogrande', 5),
      ('gsm8k', 5)
  ]

  for task, fewshot in task_and_shot:
    eval_cmd = f"""
    lm_eval --model hf \
        --model_args pretrained={model_name} \
        --tasks {task} \
        --device cpu \
        --num_fewshot {fewshot}
    """
    os.system(eval_cmd)

h6_open_llm_leaderboard(model_name="YOUR_MODEL")
